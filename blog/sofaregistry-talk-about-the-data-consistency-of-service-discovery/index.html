<!DOCTYPE html><html><head>
	<title>
	SOFARegistry | 聊一聊服务发现的数据一致性 · SOFAStack
</title>
	<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="SOFAStack is a Scalable Open Financial Architecture for building cloud native applications">

<meta name="generator" content="Hugo 0.55.5"><link rel="shortcut icon" href="https://gw.alipayobjects.com/os/q/cms/images/jqu9346l/4ba95631-2489-4885-881f-bc7f8d787d5e_w64_h61.png" type="image/png">

<link href="https://unpkg.com/purecss@1.0.0/build/base-min.css" rel="stylesheet">



<link href="/sofa2/css/main.css" rel="stylesheet" src="/sofa2/css/main.css">
<link href="/sofa2/css/zoom-image.css" rel="stylesheet" src="/sofa2/css/zoom-image.css">

<script async="" src="https://www.google-analytics.com/analytics.js"></script><script src="/sofa2/js/iconfont.js" href="/sofa2/js/iconfont.js"></script>
<script src="/sofa2/js/highlight.pack.js" href="/sofa2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script>window.SITE_LANGUAGE = "zh"</script>
<script src="/sofa2/js/app.js" href="/sofa2/js/app.js"></script>





<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-142131411-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</head>

<body><svg aria-hidden="true" style="position: absolute; width: 0px; height: 0px; overflow: hidden;"><symbol id="icon-ictick" viewBox="0 0 1024 1024"><path d="M980.96 299.904l-528.864 528.864c-24.384 24.384-61.536 28.192-89.952 11.392-5.216-3.104-10.208-6.912-14.72-11.392 0-0.032 0-0.032 0-0.032l-304.448-304.416c-28.896-28.896-28.896-75.808 0-104.704s75.744-28.896 104.672 0l252.192 252.192 476.48-476.576c28.896-28.896 75.744-28.896 104.64 0 28.928 28.896 28.928 75.808 0 104.672l0 0z"></path></symbol><symbol id="icon-search" viewBox="0 0 1024 1024"><path d="M927.104 866.816l-195.626667-208.768C780.629333 601.770667 810.666667 528.426667 810.666667 448 810.666667 271.530667 667.136 128 490.666667 128S170.666667 271.530667 170.666667 448 314.197333 768 490.666667 768c65.322667 0 126.08-19.754667 176.768-53.461333l197.461333 210.688L927.104 866.816zM256 448C256 318.592 361.258667 213.333333 490.666667 213.333333S725.333333 318.592 725.333333 448 620.074667 682.666667 490.666667 682.666667 256 577.408 256 448z"></path></symbol><symbol id="icon-menu1" viewBox="0 0 1024 1024"><path d="M225.854934 210.432687l-93.071745 0c-2.49789 0-4.523013 2.025123-4.523013 4.523013l0 93.071745c0 2.49789 2.025123 4.523013 4.523013 4.523013l93.071745 0c2.49789 0 4.523013-2.025123 4.523013-4.523013l0-93.071745C230.377948 212.45781 228.352825 210.432687 225.854934 210.432687z"></path><path d="M868.248703 210.432687 302.545594 210.432687c-15.182794 0-27.491121 2.025123-27.491121 4.523013l0 93.071745c0 2.49789 12.308327 4.523013 27.491121 4.523013L868.248703 312.550459c15.182794 0 27.491121-2.025123 27.491121-4.523013l0-93.071745C895.739824 212.45781 883.431497 210.432687 868.248703 210.432687z"></path><path d="M225.854934 461.738269l-93.071745 0c-2.49789 0-4.523013 2.025123-4.523013 4.523013l0 93.071745c0 2.49789 2.025123 4.523013 4.523013 4.523013l93.071745 0c2.49789 0 4.523013-2.025123 4.523013-4.523013l0-93.071745C230.377948 463.763392 228.352825 461.738269 225.854934 461.738269z"></path><path d="M868.248703 461.738269 302.545594 461.738269c-15.182794 0-27.491121 2.025123-27.491121 4.523013l0 93.071745c0 2.49789 12.308327 4.523013 27.491121 4.523013L868.248703 563.856042c15.182794 0 27.491121-2.025123 27.491121-4.523013l0-93.071745C895.739824 463.763392 883.431497 461.738269 868.248703 461.738269z"></path><path d="M225.854934 711.448518l-93.071745 0c-2.49789 0-4.523013 2.025123-4.523013 4.523013l0 93.071745c0 2.49789 2.025123 4.523013 4.523013 4.523013l93.071745 0c2.49789 0 4.523013-2.025123 4.523013-4.523013l0-93.071745C230.377948 713.473641 228.352825 711.448518 225.854934 711.448518z"></path><path d="M868.248703 711.448518 302.545594 711.448518c-15.182794 0-27.491121 2.025123-27.491121 4.523013l0 93.071745c0 2.49789 12.308327 4.523013 27.491121 4.523013L868.248703 813.56629c15.182794 0 27.491121-2.025123 27.491121-4.523013l0-93.071745C895.739824 713.473641 883.431497 711.448518 868.248703 711.448518z"></path></symbol><symbol id="icon-close" viewBox="0 0 1024 1024"><path d="M556.8 512l265.6-265.6c12.8-12.8 12.8-32 0-44.8s-32-12.8-44.8 0L512 467.2 246.4 201.6c-12.8-12.8-32-12.8-44.8 0s-12.8 32 0 44.8l265.6 265.6-265.6 265.6c-12.8 12.8-12.8 32 0 44.8 6.4 6.4 12.8 9.6 22.4 9.6s16-3.2 22.4-9.6l265.6-265.6 265.6 265.6c6.4 6.4 16 9.6 22.4 9.6s16-3.2 22.4-9.6c12.8-12.8 12.8-32 0-44.8L556.8 512z"></path></symbol><symbol id="icon-menu" viewBox="0 0 1024 1024"><path d="M128 298.666667h768a42.666667 42.666667 0 0 0 0-85.333334H128a42.666667 42.666667 0 0 0 0 85.333334z m768 170.666666H128a42.666667 42.666667 0 0 0 0 85.333334h768a42.666667 42.666667 0 0 0 0-85.333334z m0 256H128a42.666667 42.666667 0 0 0 0 85.333334h768a42.666667 42.666667 0 0 0 0-85.333334z"></path></symbol><symbol id="icon-ARROW" viewBox="0 0 1024 1024"><path d="M318.73024 836.32128a20.41856 20.41856 0 0 0 28.95872 0l307.2-307.2a20.45952 20.45952 0 0 0 0-28.95872l-307.2-307.2a20.45952 20.45952 0 1 0-28.95872 28.95872l292.72064 292.72064-292.72064 292.72064a20.45952 20.45952 0 0 0 0 28.95872z" fill="#231F20"></path></symbol><symbol id="icon-copy" viewBox="0 0 1024 1024"><path d="M877.714286 0H265.142857c-5.028571 0-9.142857 4.114286-9.142857 9.142857v64c0 5.028571 4.114286 9.142857 9.142857 9.142857h566.857143v786.285715c0 5.028571 4.114286 9.142857 9.142857 9.142857h64c5.028571 0 9.142857-4.114286 9.142857-9.142857V36.571429c0-20.228571-16.342857-36.571429-36.571428-36.571429zM731.428571 146.285714H146.285714c-20.228571 0-36.571429 16.342857-36.571428 36.571429v606.514286c0 9.714286 3.885714 18.971429 10.742857 25.828571l198.057143 198.057143c2.514286 2.514286 5.371429 4.571429 8.457143 6.285714v2.171429h4.8c4 1.485714 8.228571 2.285714 12.571428 2.285714H731.428571c20.228571 0 36.571429-16.342857 36.571429-36.571429V182.857143c0-20.228571-16.342857-36.571429-36.571429-36.571429zM363.428571 950.857143h-0.228571L192 779.657143v-0.228572h171.428571v171.428572z"></path></symbol><symbol id="icon-transfer" viewBox="0 0 1024 1024"><path d="M769.841131 507.345455a31.340606 31.340606 0 0 0-44.187151 0 30.947556 30.947556 0 0 0 0 43.959596l179.613737 178.424242H43.618263a31.030303 31.030303 0 1 0 0 62.060606h861.390868L725.385051 970.472727a30.947556 30.947556 0 0 0 0 43.959596c6.237091 6.206061 14.03604 9.050505 22.093575 9.050505 8.067879 0 16.115071-3.10303 22.093576-9.050505l232.892768-231.692929a30.616566 30.616566 0 0 0 9.102222-21.979798c0-8.274747-3.382303-16.290909-9.102222-21.979798L769.861818 507.345455zM254.158869 516.654545c6.237091 6.206061 14.025697 9.050505 22.093575 9.050506 8.057535 0 16.115071-3.10303 22.093576-9.050506a30.947556 30.947556 0 0 0 0-43.959596L118.732283 294.012121h861.401212a31.030303 31.030303 0 0 0 31.185454-31.030303c0-17.066667-14.03604-31.030303-31.195797-31.030303H118.990869L298.614949 53.268687a30.947556 30.947556 0 0 0 0-43.959596 31.340606 31.340606 0 0 0-44.187151 0L21.514343 241.00202a30.947556 30.947556 0 0 0 0 43.959596l232.623839 231.692929z" fill="#2E323F"></path></symbol><symbol id="icon-arrow" viewBox="0 0 1024 1024"><path d="M20.48 245.76h983.04L512 778.24z"></path></symbol></svg>
	<header class="ss-header">
	<nav class="navbar" role="navigation" aria-label="main navigation">
		<div class="navbar-brand">
			<a class="logo-link" href="/sofa2/">
				<img class="logo" src="/sofa2/img/logo.png" href="/sofa2/img/logo.png">
			</a>
			<div class="-show-mobile">
				<a id="mobile-menu-icon">
					<svg class="icon" aria-hidden="true">
						<use xlink:href="#icon-menu"></use>
					</svg>
				</a>
				<nav id="mobile-menu">
						<div id="js-menu-search-mobile" class="navbar-search-mobile">
							<input class="input" placeholder="请输入要搜索的关键词">
							<svg class="icon" aria-hidden="true">
								<use xlink:href="#icon-search"></use>
							</svg>
						</div>
					
          
            <a class="" href="/sofa2/projects/">
              <span>
                项目
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
            <a class="" href="/sofa2/guides/">
              <span>
                指南
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
            <a class="" href="/sofa2/blog/">
              <span>
                博客
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
            <a class="" href="/sofa2/activities/">
              <span>
                活动
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
            <a class="" href="/sofa2/community/">
              <span>
                社区
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
            <a class="" href="/sofa2/awesome/">
              <span>
                Awesome SOFA
              </span>
              <svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"></use></svg>
            </a>
					
					
				</nav>
			</div>
		</div>

		<div class="navbar-menu -hidden-mobile">
			<div class="navbar-start">
				
				
					<a class="navbar-item " href="/sofa2/projects/">项目</a>
				
					<a class="navbar-item " href="/sofa2/guides/">指南</a>
				
					<a class="navbar-item " href="/sofa2/blog/">博客</a>
				
					<a class="navbar-item " href="/sofa2/activities/">活动</a>
				
					<a class="navbar-item " href="/sofa2/community/">社区</a>
				
					<a class="navbar-item " href="/sofa2/awesome/">Awesome SOFA</a>
				
			</div>
			<div class="navbar-end">
				<div class="navbar-item">
					<div id="js-menu-search" class="navbar-search">
						<input class="input" placeholder="请输入要搜索的关键词">
						<svg class="icon" aria-hidden="true">
							<use xlink:href="#icon-search"></use>
						</svg>
					</div>
				</div>
				<div class="navbar-item">
					
				</div>
			</div>
		</div>
	</nav>
</header>


	

	

	

	

	

	



	

<div class="ss-layout-container">
	<main class="ss-layout-main -card">
		<div class="ss-meta">
	<h1 class="title">
		SOFARegistry | 聊一聊服务发现的数据一致性
	</h1>
	<div class="meta">
		
			2023-01-03 ·
			
				<a href="#"></a> ·
			
			<span class="tags">
				
					<a class="tag" href="/sofa2/tags/sofastack/" rel="tag">#SOFAStack</a>
				
			</span>
		
	</div>
</div>
		<article class="typo">
	<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/19546cb0b5394cfa85db3cca72ad65e9~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<p>文｜肖健（花名：昱恒）</p>

<p>蚂蚁集团技术专家</p>

<p><em>专注于服务发现领域，目前主要从事蚂蚁注册中心 SOFARegistry 设计、研发工作。</em></p>

<p>本文 <strong>9492</strong>字 阅读 <strong>24</strong> 分钟</p>

<h2 id="part-1-前言">PART. 1 前言</h2>

<h3 id="1-1-什么是服务发现">1.1 什么是服务发现</h3>

<p>在微服务的体系中，多个应用程序之间将以 RPC 方式进行相互通信。这些应用程序的服务实例是动态变化的，我们需要知道这些实例的准确列表，才能让应用程序之间按预期进行 RPC 通信。这就是服务发现在微服务体系中的核心作用。</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/694f7502e94a4facaffc0bc7fcac2cc0~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<p>SOFARegistry 是蚂蚁集团在生产大规模使用的服务注册中心，经历了多年大促的考验，支撑蚂蚁庞大的服务集群；具有分布式可水平扩容、容量大、推送延迟低、高可用等特点。</p>

<h3 id="1-2-服务发现的考量">1.2 服务发现的考量</h3>

<p>设计和考量一个服务发现系统，可以从下面这些指标展开：</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/54ad22bcff534f9ca1065d7fe9720da3~tplv-k3u1fbpfcp-watermark.image?" alt="image.png" data-action="zoom"></p>

<p>各个指标之间并不是相互独立的。例如对于数据一致性方案的选型也会影响到数据分区、数据复制、集群容灾、多集群同步等方案的决策，也在很大程度上决定这个服务发现系统的整体架构。</p>

<p>这篇文章重点分析了各个服务发现系统的数据一致性方案，以及基于这个方案延伸出来的特性，帮助大家初步了解服务发现系统。</p>

<h2 id="part-2-开源产品分析">PART. 2 开源产品分析</h2>

<h3 id="2-1-为什么需要数据一致性">2.1 为什么需要数据一致性</h3>

<p>根据上述描述，数据一致性在服务发现系统中如此重要，甚至会影响到整个服务发现系统的各方面架构考量，那我们到底为什么需要数据一致性呢？</p>

<p>要回答这个问题，让我们从单点故障说起：早期我们使用的服务，以及服务数据存储，它们往往是部署在单节点上的。但是单节点存在单点故障，一旦单节点宕机就整个服务不可用，对业务影响非常大。随后，为了解决单点问题，软件系统引入了数据复制技术，实现多副本。</p>

<p>通过数据复制方案，一方面我们可以提高服务可用性，避免单点故障；另一方面，多副本可以提升读吞吐量、甚至就近部署在业务所在的地理位置，降低访问延迟。</p>

<p>随着多副本的引入，就会涉及到多个副本之间的数据怎样保持一致的问题，于是数据一致性随之而来。</p>

<p><strong>2.2 开源产品分析</strong></p>

<p>对于多个副本之间进行数据同步，一致性关系从强到弱依次是：</p>

<ul>
<li><p>线性一致性 <em>（Linearizability consistency）</em></p></li>

<li><p>顺序一致性 <em>（Sequential consistency）</em></p></li>

<li><p>因果一致性 <em>（Causal consistency）</em></p></li>

<li><p>最终一致性 <em>（Eventual consistency）</em></p></li>
</ul>

<p>我们对比一下目前开源的比较典型的服务发现产品，在数据一致性上的方案实现：</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/660e84f999c44b219d6b12cfe4835292~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<h2 id="part-3-etcd-数据一致性">PART. 3 Etcd 数据一致性</h2>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a3193aa69614f44841658a1b99551f1~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<h3 id="3-1-etcd-读数据流程">3.1 Etcd 读数据流程</h3>

<p><strong>1. Client</strong>：Etcdctl 封装了操作 Etcd、KV Server、Cluster、Auth、Lease、Watch 等模块的 API；</p>

<p><strong>2. KV Server：</strong> Client 发送 RPC 请求到了 Server 后，KV Server 基于拦截器记录所有请求的执行耗时及错误码、来源 IP 等，也可控制请求是否允许通过；</p>

<p><strong>3. Raft：</strong> Etcd 收到读请求后，向 Etcd Raft 模块发起 Read Index 读数据请求，返回最新的 ReadState 结构体数据；</p>

<p><strong>4. MVCC</strong>：KV Server 获取到 Read State 数据后，从 MVCC 模块的 Tree Index 读取基于 Key-Version 的唯一标识 Revision；再以 Revision 作为 Key 从 Boltdb 中读取数据。</p>

<h3 id="3-2-etcd-写数据流程">3.2 Etcd 写数据流程</h3>

<p><strong>1. Client：</strong> Etcdctl 封装了操作 Etcd、KV Server、Cluster、Auth、Lease、Watch 等模块的 API；</p>

<p><strong>2. KV Server：</strong> 通过一系列检查之后，然后向 Raft 模块发起 <em>（Propose）</em> 一个提案 <em>（Proposal）</em> ，提案内容为存储的 value；</p>

<p><strong>3. Raft：</strong></p>

<p>a.向 Raft 模块发起提案后，KV Server 模块会等待此 put 请求；如果一个请求超时未返回结果，会出现的 EtcdServer：request timed out 错误。</p>

<p>b.Raft 模块收到提案后，如果当前节点是 Follower，它会转发给 Leader，只有 Leader 才能处理写请求。Leader 收到提案后，通过 Raft 模块将 put 提案消息广播给集群各个节点，同时需要把集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到一个 WAL <em>（Write Ahead Log）</em> 日志文件中，用于保证集群的一致性、可恢复性。</p>

<p><strong>4.</strong> Raft 模块提交 Proposal 完成后，向&nbsp;<strong>MVCC</strong>&nbsp;模块提交写数据。</p>

<h3 id="3-3-raft-功能分解">3.3 Raft 功能分解</h3>

<p>共识算法的祖师爷是 Paxos， 但是由于它过于复杂、难于理解，工程实践上也较难落地，导致在工程界落地较慢。</p>

<p>Standford 大学的 Diego 提出的 Raft 算法正是为了可理解性、易实现而诞生的，它通过问题分解，将复杂的共识问题拆分成三个子问题，分别是：</p>

<ul>
<li><p><strong>Leader 选举：</strong> Leader 故障后集群能快速选出新 Leader；</p></li>

<li><p><strong>日志复制</strong>：集群只有 Leader 能写入日志， Leader 负责复制日志到 Follower 节点，并强制 Follower 节点与自己保持相同；</p></li>

<li><p><strong>安全性：</strong> 一个任期内集群只能产生一个 Leader、已提交的日志条目在发生 Leader 选举时，一定会存在更高任期的新 Leader 日志中、各个节点的状态机应用的任意位置的日志条目内容应一样等。</p></li>
</ul>

<p>下面以实际场景为案例，分别深入讨论这三个子问题，看看 Raft 是如何解决这三个问题，以及在 Etcd 中的应用实现。</p>

<p>关于 Raft 的 Leader 选举与日志复制，可以从&nbsp;*<a href="#">http://www.kailing.pub/raft/index.html</a>*&nbsp;动画中进一步了解。</p>

<h3 id="3-4-etcd-读写一致性">3.4 Etcd 读写一致性</h3>

<h4 id="3-4-1-线性一致性写">3.4.1 线性一致性写</h4>

<p>所有的 Read/Write 都会来到 Leader，Write 会有 Oplog Leader 被序列化，依次顺序往后 commit，并 Apply 然后在返回，那么一旦一个 Write 被 committed，那么其前面的 Write 的 Oplog 一定就被 committed 了。所有的 Write 都是有严格的顺序的，一旦被 committed 就可见了，所以 Raft 是线性一致性写。</p>

<h4 id="3-4-2-线性一致性读">3.4.2 线性一致性读</h4>

<p>Etcd 默认的读数据流程是 Linearizability Read，那么怎么样才能读取到 Leader 已经完成提交的数据呢？</p>

<p><strong>读请求走一遍 Raft 协议</strong></p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48b3d412408c4d90b1be461bad4accf4~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<p>每个 Read 都生成一个对应的 Oplog，和 Write 一样，都会走一遍一致性协议的流程，会在此 Read Oplog 被 Apply 的时候读，那么这个 Read Oplog 之前的 Write Oplog 肯定也被 Applied 了，那么一定能够被读取到，读到的也一定是最新的。</p>

<ul>
<li><p>有什么问题?</p></li>

<li><p>不仅有日志写盘开销，还有日志复制的 RPC 开销，在读比重较大的系统中是无法接受的；</p></li>

<li><p>还多了一堆的 Raft ‘读日志’。</p></li>
</ul>

<p><strong>Read Index</strong></p>

<ul>
<li><p>这是 Raft 论文中提到过的一种优化方案，具体来说：</p></li>

<li><p>Leader 将当前自己 Log 的 Commit Index 记录到一个 local 变量 Read Index 里面；</p></li>

<li><p>向其它节点发起一次 Heartbeat，如果大多数节点返回了对应的 Heartbeat Response，那么 Leader 就能够确定现在自己仍然是 Leader；</p></li>

<li><p>Leader 等待自己的状态机执行，直到 Apply Index 超过了 Read Index，这样就能够安全的提供 Linearizable Read 了；</p></li>

<li><p>Leader 执行 Read 请求，将结果返回给 Client。</p></li>

<li><p>Read Index 小结：</p></li>
</ul>

<ol>
<li><p>相比较于走 Raft Log 的方式，Read Index 读省去了磁盘的开销，能大幅度提升吞吐，结合 JRaft 的 batch + pipeline ACK + 全异步机制，三副本的情况下 Leader 读的吞吐接近于 RPC 的上限；</p></li>

<li><p>延迟取决于多数派中最慢的一个 Heartbeat Response。</p></li>
</ol>

<p><strong>Lease Read</strong></p>

<ul>
<li><p>Lease Read 与 Read Index 类似，但更进一步，不仅省去了 Log，还省去了网络交互；它可以大幅提升读的吞吐，也能显著降低延时；</p></li>

<li><p>基本的思路是 Leader 取一个比 election timeout <em>（1s）</em> 小的租期 <em>(最好小一个数量级，100ms)</em> ， 在租约期内不会发生选举，这就确保了 Leader 不会变，所以可以跳过 Read Index 的第二步，也就降低了延时。</p></li>
</ul>

<h4 id="3-4-3-串行性读">3.4.3 串行性读</h4>

<p>直接读状态机数据返回、无需通过 Raft 协议与集群进行交互的模式，在 Etcd 里叫做串行 <em>(Serializable)</em> 读，可以通过 WithSerializable() 进行设置，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dba18e0aaeec4fbabda6ca138f256c13~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<h2 id="part-4-eureka-数据一致性">PART. 4 Eureka 数据一致性</h2>

<h3 id="4-1eureka-数据读写流程">4.1Eureka 数据读写流程</h3>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e5c25c0ff3a247ea9b448d85b5d6beeb~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<ul>
<li><p>Eureka 节点完全对等部署，每台 Server 保存全量的数据：</p></li>

<li><p>Sub 会定时 <em>（Eureka.client.registry-fetch-interval-seconds 定义，默认值为 30s）</em> 向注册中心获取数据，更新本地缓存；</p></li>

<li><p>服务实例会通过心跳&nbsp; <em>(Eureka.Instance.lease-renewal-interval-in-seconds 定义心跳的频率，默认值为 30s)</em> 续约的方式向 Eureka Server 定时更新自己的状态。Eureka Server 收到心跳后，会通知集群里的其它 Eureka Server&nbsp; 更新此实例的状态。Service Provider/Service Consumer 也会定时更新缓存的实例信息。</p></li>

<li><p>服务的下线有两种情况：</p></li>

<li><p>在 Service Provider 服务 shutdown 的时候，主动通知 Eureka Server 把自己剔除，从而避免客户端调用已经下线的服务；</p></li>

<li><p>Eureka Server 会定时 <em>（间隔值是 Eureka.server.eviction-interval-timer-in-ms，默认值为 0，默认情况不删除实例）</em> 进行检查，如果发现实例在在一定时间 <em>（此值由 Eureka.Instance.lease-expiration-duration-in-seconds 定义，默认值为 90s）</em> 内没有收到心跳，则会注销此实例。</p></li>
</ul>

<h3 id="4-2-启动全量拉取">4.2 启动全量拉取</h3>

<pre><code class="language-Java hljs"><span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">fetchRegistry</span><span class="hljs-params">()</span> 
</span>{
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;If&nbsp;the&nbsp;delta&nbsp;is&nbsp;disabled&nbsp;or&nbsp;if&nbsp;it&nbsp;is&nbsp;the&nbsp;first&nbsp;time,get&nbsp;all&nbsp;applications    </span>
    <span class="hljs-keyword">if</span> (serverConfig.shouldDisableDeltaForRemoteRegions()       
    || (getApplications() == <span class="hljs-keyword">null</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    ||(getApplications().getRegisteredApplications().size()&nbsp;==&nbsp;<span class="hljs-number">0</span>))&nbsp;{       
    <span class="hljs-comment">// 全量获取&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
    logger.info(<span class="hljs-string">"Disable&nbsp;delta&nbsp;property&nbsp;:&nbsp;
    }
    "</span>,serverConfig.shouldDisableDeltaForRemoteRegions());
    logger.info(<span class="hljs-string">"Application&nbsp;is&nbsp;null&nbsp;:&nbsp;{}"</span>,getApplications()&nbsp;==&nbsp;<span class="hljs-keyword">null</span>);
    logger.info(<span class="hljs-string">"Registered&nbsp;Applications&nbsp;size&nbsp;is&nbsp;zero&nbsp;:{}"</span>,&nbsp;getApplications().getRegisteredApplications().isEmpty()); 
    success = storeFullRegistry();
    } 
    <span class="hljs-keyword">else</span> {
    <span class="hljs-comment">//增量获取        success = fetchAndStoreDelta();    </span>
    }
    <span class="hljs-keyword">return</span> success;
    }
</code></pre>

<ol>
<li><p>Eureka-Server 的复制算法是依赖增量复制+全量复制实现的。区别于 ZooKeeper，这里没有 Leader 的概念，所有的结点都是平等的，因此数据并不保证一致性。</p></li>

<li><p>启动时调用 storeFullRegistry，<strong>选取 1 台 Eureke-Server 进行一次全量拉取</strong>，使用 EurekaHttpClient.getApplications()；url=“/apps” ；</p></li>

<li><p>Server 端获取本地 Cache 中的数据进行返回。</p></li>
</ol>

<h3 id="4-3-数据变更增量复制">4.3 数据变更增量复制</h3>

<h3 id="4-3-1-client-发起复制">4.3.1 Client 发起复制</h3>

<ol>
<li><p>此处的 Client 指的是 Eureka-1，当 Eureka-1 收到客户端的服务注册 <em>（Registers）</em> 、服务更新 <em>（Renewals）</em> 、服务取消 <em>（Cancels）</em> 、服务超时 <em>（Expirations）</em> 和服务状态变更 <em>（Status Changes）</em> 后，刷新本地注册信息；</p></li>

<li><p>遍历所有的节点 <em>（会排除自己）</em> ，将消息转发到其它节点；为了实现数据同步 <em>（Eureka 保证的 AP 特性）</em> ，每个节点需要维护一个节点列表，这个节点列表就是 PeerEurekaNodes，她负责管理所有的 PeerEurekaNodes；</p></li>

<li><p>转发请求时，在 HTTP&nbsp;Header 中携带 x-netflix-discovery-replication : true 的标识，则处理请求的机器不会再将请求继续转发，避免死循环。</p></li>
</ol>

<pre><code class="language-Java hljs"><span class="hljs-comment">/**&nbsp;
*&nbsp;
Replicates&nbsp;all&nbsp;instance&nbsp;changes&nbsp;to&nbsp;peer&nbsp;Eureka&nbsp;nodesexcept&nbsp;for
* replication traffic to this node. * 
*/</span><span class="hljs-function"><span class="hljs-keyword">private</span>&nbsp;<span class="hljs-keyword">void</span>&nbsp;<span class="hljs-title">replicateInstanceActionsToPeers</span><span class="hljs-params">(Action&nbsp;action,String&nbsp;appName,&nbsp;&nbsp;
String&nbsp;id,InstanceInfo&nbsp;info,&nbsp;InstanceStatus&nbsp;newStatus,&nbsp;&nbsp;&nbsp;
PeerEurekaNodenode)</span>&nbsp;</span>{
    <span class="hljs-keyword">switch</span> (action) {  
    <span class="hljs-keyword">case</span> Cancel:            
    node.cancel(appName, id);
    <span class="hljs-keyword">break</span>;
    <span class="hljs-keyword">case</span> Heartbeat:
    InstanceStatus&nbsp;overriddenStatus&nbsp;=overriddenInstanceStatusMap.get(id);
    infoFromRegistry&nbsp;=getInstanceByAppAndId(appName,&nbsp;id,&nbsp;<span class="hljs-keyword">false</span>);&nbsp;&nbsp;
    node.heartbeat(appName,&nbsp;id,&nbsp;infoFromRegistry,overriddenStatus,&nbsp;<span class="hljs-keyword">false</span>);  
    <span class="hljs-keyword">break</span>;
    <span class="hljs-keyword">case</span> Register: 
    node.register(info);   
    <span class="hljs-keyword">break</span>;    
    <span class="hljs-keyword">case</span> StatusUpdate:&nbsp;&nbsp;&nbsp;
    infoFromRegistry&nbsp;=getInstanceByAppAndId(appName,&nbsp;id,&nbsp;<span class="hljs-keyword">false</span>);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    node.statusUpdate(appName,&nbsp;id,&nbsp;newStatus,infoFromRegistry);
    <span class="hljs-keyword">break</span>;     
    <span class="hljs-keyword">case</span> DeleteStatusOverride:&nbsp;&nbsp;
    infoFromRegistry&nbsp;=getInstanceByAppAndId(appName,&nbsp;id,&nbsp;<span class="hljs-keyword">false</span>);&nbsp;&nbsp;&nbsp;
    node.deleteStatusOverride(appName,&nbsp;id,infoFromRegistry);   
    <span class="hljs-keyword">break</span>; 
    }}
    <span class="hljs-meta">@Overridepublic</span>&nbsp;<span class="hljs-function">EurekaHttpResponse&lt;Void&gt;&nbsp;<span class="hljs-title">register</span><span class="hljs-params">(InstanceInfo&nbsp;info)</span></span>{
    String urlPath = <span class="hljs-string">"apps/"</span> + 
    info.getAppName();&nbsp;
    Builder&nbsp;resourceBuilder&nbsp;=jerseyClient.target(serviceUrl).path(urlPath).request();    
    addExtraProperties(resourceBuilder);  
    addExtraHeaders(resourceBuilder);  
    response = resourceBuilder      
    .accept(MediaType.APPLICATION_JSON)
    .acceptEncoding(<span class="hljs-string">"gzip"</span>)       
    .post(Entity.json(info));&nbsp;&nbsp;
    returnanEurekaHttpResponse(response.getStatus()
    ).headers(headersOf(response)).build();
}
</code></pre>

<h3 id="4-3-2-server-处理增量复制">4.3.2 Server 处理增量复制</h3>

<p>1.&nbsp;Server 收到数据变更请求后，根据 lastDirtyTimestamp 处理数据版本冲突，lastDirtyTimestamp 是注册中心里面服务实例 <em>（Instance）</em> 的一个属性，表示此服务实例最近一次变更时间；</p>

<ol>
<li>Eureka Server A 把数据发送给 Eureka Server B，数据冲突有 2 种情况：</li>
</ol>

<ul>
<li><p>A 的数据比 B 的新，B 返回 404，A 重新把这个应用实例注册到 B；</p></li>

<li><p>A 的数据比 B 的旧，B 返回 409，要求 A 同步 B 的数据。</p></li>
</ul>

<pre><code class="language-Java hljs"><span class="hljs-function"><span class="hljs-keyword">public</span>&nbsp;<span class="hljs-keyword">void</span>&nbsp;<span class="hljs-title">register</span>
<span class="hljs-params">(InstanceInfo&nbsp;registrant,&nbsp;<span class="hljs-keyword">int</span>&nbsp;leaseDurtion,&nbsp;<span class="hljs-keyword">boolean</span>&nbsp;isReplication)</span>
</span>{
    <span class="hljs-comment">// .... 获取 instance 实例对象</span>
    Lease&lt;InstanceInfo&gt;&nbsp;existingLease&nbsp;=gMap.get(registrant.getId()); 
    <span class="hljs-comment">//如果 Eureka Server 中该实例已经存在&nbsp;</span>
    <span class="hljs-keyword">if</span>&nbsp;(existingLease&nbsp;!=&nbsp;<span class="hljs-keyword">null</span>&nbsp;&amp;&amp;&nbsp;(existingLease.getHolder()!=&nbsp;<span class="hljs-keyword">null</span>))
    {     
    <span class="hljs-comment">// 比较 lastDirtyTimestamp ， 以 lastDirtyTimestamp 大的为准&nbsp;&nbsp;&nbsp;</span>
    <span class="hljs-keyword">if</span>&nbsp;(existingLastDirtyTimestamp&nbsp;&gt;registrationLastDirtyTimestamp)&nbsp;{    
    registrant = existingLease.getHolder();  
    }
    }}
</code></pre>

<h3 id="4-4-apps-定时增量同步与校验">4.4 Apps 定时增量同步与校验</h3>

<h4 id="4-4-1-client">4.4.1 Client</h4>

<p>在 Eureka Server 启动完成初次全量同步后，考虑从增量数据复制会有处理失败的情况，所以需要有一个定时任务每隔 30s 进行增量数据同步与校验：</p>

<pre><code class="language-Java hljs"><span class="hljs-meta">@Overridepublic</span>&nbsp;<span class="hljs-function">EurekaHttpResponse&lt;Applications&gt;
<span class="hljs-title">getDelta</span><span class="hljs-params">(String...regions)</span>&nbsp;
</span>{    
<span class="hljs-keyword">return</span> getApplicationsInternal(<span class="hljs-string">"apps/delta"</span>, regions);
}
<span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">fetchAndStoreDelta</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Throwable 
</span>{    
<span class="hljs-keyword">long</span> currGeneration = fetchRegistryGeneration.get(); 
Applications delta = fetchRemoteRegistry(<span class="hljs-keyword">true</span>);
   String reconcileHashCode = <span class="hljs-string">""</span>; 
   <span class="hljs-comment">//加锁进行差量更新    </span>
   <span class="hljs-keyword">if</span> (fetchRegistryUpdateLock.tryLock())
   {        
   <span class="hljs-keyword">try</span> 
   {            updateDelta(delta);&nbsp;&nbsp;
   reconcileHashCode&nbsp;=getApplications().getReconcileHashCode(); 
   } 
   <span class="hljs-keyword">finally</span> {  
   fetchRegistryUpdateLock.unlock();
   }    
   } 
   <span class="hljs-keyword">else</span> {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; logger.warn(<span class="hljs-string">"Cannot&nbsp;acquire&nbsp;update&nbsp;lock,&nbsp;aborting&nbsp;udateDelta&nbsp;operation&nbsp;of&nbsp;fetchAndStoreDelta"</span>); 
   }
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;There&nbsp;is&nbsp;a&nbsp;diff&nbsp;in&nbsp;number&nbsp;of&nbsp;instances&nbsp;for&nbsp;somereason&nbsp;</span>
    <span class="hljs-keyword">if</span>&nbsp;(!reconcileHashCode.equals(delta.getAppsHashCode()))
    {        deltaMismatches++;&nbsp;
    <span class="hljs-keyword">return</span>&nbsp;reconcileAndLogDifference(delta,reconcileHashCode); 
    } 
    <span class="hljs-keyword">else</span> {
    deltaSuccesses++;
    }
    <span class="hljs-keyword">return</span> delta != <span class="hljs-keyword">null</span>;
    }
</code></pre>

<ol>
<li><p>增量数据同步成功后加锁，进行 add、modify、 delete 等操作，url=“apps/delta”；</p></li>

<li><p>使用 updateDelta 更新数据后，使用 reconcileHashCode <em>（根据 Client 和 Server 的全量 Applications 计算获得）</em> 校验是否增量更新成功，reconcileHashCode 格式：UP_count1_DOWN_count2_STARTING_count3；</p></li>

<li><p>如果校验的 reconcileHashCode 不一致，再发起一次全量同步动作；</p></li>
</ol>

<h3 id="4-4-2-server">4.4.2 Server</h3>

<pre><code class="language-Java hljs"><span class="hljs-keyword">private</span>&nbsp;ConcurrentLinkedQueue&lt;RecentlyChangedItem&gt;&nbsp;
recentlyChangedQueue&nbsp;=&nbsp;newConcurrentLinkedQueue&lt;RecentlyChangedItem&gt;();<span class="hljs-meta">@Deprecatedpublic</span> <span class="hljs-function">Applications <span class="hljs-title">getApplicationDeltas</span><span class="hljs-params">()</span> 
</span>{    
        <span class="hljs-comment">//从ecentlyChangedQueue获取增量同步的数据&nbsp;&nbsp;&nbsp;&nbsp;</span>
Iterator&lt;RecentlyChangedItem&gt;&nbsp;iter&nbsp;=<span class="hljs-keyword">this</span>.recentlyChangedQueue.iterator();
logger.debug(<span class="hljs-string">"The&nbsp;number&nbsp;of&nbsp;elements&nbsp;in&nbsp;the&nbsp;delta&nbsp;queueis&nbsp;:&nbsp;{}"</span>,  
<span class="hljs-keyword">this</span>.recentlyChangedQueue.size()); 
<span class="hljs-keyword">while</span> (iter.hasNext()) 
{&nbsp;&nbsp;&nbsp;&nbsp;
Lease&lt;InstanceInfo&gt;&nbsp;lease&nbsp;=iter.next().getLeaseInfo();    
InstanceInfo instanceInfo = lease.getHolder();        logger.debug(&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<span class="hljs-string">"The&nbsp;instance&nbsp;id&nbsp;{}&nbsp;is&nbsp;found&nbsp;with&nbsp;status&nbsp;{}&nbsp;andactiontype&nbsp;{}"</span>,&nbsp;&nbsp;&nbsp;
instanceInfo.getId(),instanceInfo.getStatus().name(),&nbsp;instanceInfo.getActionType().name());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Application&nbsp;app&nbsp;=&nbsp;applicationInstancesMap.get(instaceInfo.getAppName());
<span class="hljs-keyword">if</span> (app == <span class="hljs-keyword">null</span>) 
{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
app&nbsp;=&nbsp;newApplication(instanceInfo.getAppName());&nbsp;&nbsp;
applicationInstancesMap.put(instanceInfo.getAppName(),&nbsp;app);   
apps.addApplication(app);        
}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
app.addInstance(newInstanceInfo(decorateInstanceInfo(lease)));
}    
<span class="hljs-comment">//计算本地全量数据的 hashcode    </span>
apps.setAppsHashCode(allApps.getReconcileHashCode());
}
</code></pre>

<ol>
<li><p>从 recentlyChangedQueue 队列中获取增量数据，根据方法的注释，recentlyChangedQueue 中存放的是 getRetentionTimeInMSInDeltaQueue 时间内 <em>（默认 180s）</em> 的 Client 注册信息；</p></li>

<li><p>Client 发起 Delta 增量同步时，前后两次请求可能获取到相同的 Delta Apps 信息，Client 需要兼容这种情况；</p></li>

<li><p>Eureka Server 收到 Register、Cancel、StatusUp、Expirations 等操作时，会更新 recentlyChangedQueue 中的信息；</p></li>

<li><p>设置定时任务 <em>（30s 运行一次）</em> 清理队列中的过期数据 <em>（180s）</em> 。</p></li>
</ol>

<h3 id="4-5-点评">4.5 点评</h3>

<ol>
<li><p>Client 30s 向服务端获取一次数据，Service 变化生效时间较长；</p></li>

<li><p>使用 recentlyChangedQueue 保存 180s 数据变更的方式进行增量同步，如果数据量大队列容易爆炸；</p></li>

<li><p>如果 reconcileHashCode 在增量同步的时候计算不一致，发起全量同步，如果全量同步的次数太多，容易有性能瓶颈；</p></li>

<li><p>reconcileHashCode 格式：UP_count1_DOWN_count2_STARTING_count3，只是确保 UP/DOWN 数量相等，无法保证数据是最终一致性。</p></li>
</ol>

<h2 id="part-5-nacos-数据一致性">PART. 5 Nacos 数据一致性</h2>

<h3 id="5-1-nacos-数据读写流程">5.1 Nacos 数据读写流程</h3>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5db66958f824416fbd1fb09dbdc2ed77~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<ol>
<li><p>Nacos 使用的是单节点全量存储数据，Client 与单个 Nacos 节点进行服务的发布和订阅；</p></li>

<li><p>每个 Server 中有一个请求处理的前置 Filter，根据 Server 列表的 Hash 分片，计算 Pub 数据归属于哪台 Nacos-Server，然后进行请求转发；</p></li>

<li><p>Nacos-1 中调用本地的 Register 方法，将服务信息存储到本地内存的服务注册列表，然后给 Client 返回成功；</p></li>

<li><p>Nacos-1 根据 Distro 协议，将 Pub Register 请求同步给全集群的 Nacos Server；</p></li>

<li><p>Sub Client 连接到 Nacos-3 进行服务数据订阅，Nacos-3 将本地数据进行返回。</p></li>
</ol>

<h3 id="5-2-启动全量拉取">5.2 启动全量拉取</h3>

<ol>
<li><p>新加入的 Distro 节点会进行全量数据拉取，具体操作是轮询所有的 Distro 节点，通过向其它的机器发送请求拉取全量数据；</p></li>

<li><p>Nacos v1 基于 HTTP 协议进行通信，v2 基于 gRPC 协议进行通信；</p></li>

<li><p>启动期间需要向全量的 Distro 机器都发起全量拉取：</p></li>
</ol>

<ul>
<li><p>对于新的机器，从处理读请求的角度看，可以只拉取 1 台&nbsp;Distro 的机器数据，即使获取的部分数据是比较旧的，也只是与拉取的 Target Server 提供了相同的数据服务；</p></li>

<li><p>从处理写请求的角度看，只有从全量的机器拉取，才能确保本机器负责的 Hash 分片的数据最新，所以需要向所有的 Distro Server 做数据同步，确保本机负责的 Hash 分片的数据最新；</p></li>

<li><p>在全量拉取操作完成之后，Nacos 的每台机器上都维护了当前的所有注册上来的非持久化实例数据，开始提供服务。</p></li>
</ul>

<h3 id="5-3-数据变更增量复制">5.3 数据变更增量复制</h3>

<ul>
<li>对于 add、change、delete，在 Nacos-1 执行后，将数据变更与 action 广播到全集群的 Distro 服务器；  有了上述两个机制之后，也不能完全确保 Distro 服务器之间的数据完全是相同的，例如存在 Notify 失败等场景。因此还需要有一个定时校验机制，比对全集群的 Server 之间的数据一致性，并进行修复。</li>
</ul>

<h3 id="5-4-v1-版本节点数据-verify">5.4 v1 版本节点数据&nbsp;Verify</h3>

<ul>
<li><p>Nacos-1 每隔 5s 执行一次定时任务，计算本节点数据的 digest 摘要；</p></li>

<li><p>Verify 校验时，将本地的所有 Service，根据 Hash 规则匹配本节点负责的 Service，并计算对应的 CheckSum，然后组装成请求参数：Map<string< ne-text="">,String&gt; keyCheckSums 。</string<></p></li>

<li><p>CheckSum 的计算规则如下：</p></li>
</ul>

<pre><code class="language-Java hljs"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title">recalculateChecksum</span><span class="hljs-params">()</span> </span>{ 
List&lt;Instance&gt; ips = allIPs();
    StringBuilder ipsString = <span class="hljs-keyword">new</span> StringBuilder(); 
    String serviceString = getServiceString();   
    ipsString.append(serviceString);
    <span class="hljs-keyword">for</span> (Instance ip : ips) {   
    String string = ip.getIp() + <span class="hljs-string">":"</span> + ip.getPort() + <span class="hljs-string">"_"</span> + ip.getWeight() + <span class="hljs-string">"_"</span> + ip.isHealthy() + <span class="hljs-string">"_"</span> + ip   
    .getClusterName();     
    ipsString.append(string); 
    ipsString.append(<span class="hljs-string">','</span>);    }
    checksum = MD5Utils.md5Hex(ipsString.toString(), Constants.ENCODE);
    }
</code></pre>

<ul>
<li><p>Nacos-2 Server 端收到 Verify 请求后，将数据分成 3 种场景：不需要处理的、需要更新的、需要删除的；</p></li>

<li><p>对于需要删除的 Service 数据，直接在内存中删除；</p></li>

<li><p>对于需要更新的 Service，调用 Nacos-1 进行 Server 的全量数据获取，然后更新本地的数据。</p></li>
</ul>

<pre><code class="language-Java hljs"><span class="hljs-comment">// 对于有差异的 service 进行全量数据同步@Overridepublic DistroData getData(DistroKey key, String targetServer) </span>
{    
<span class="hljs-keyword">try</span> {  
List&lt;String&gt; toUpdateKeys = <span class="hljs-keyword">null</span>;    
<span class="hljs-keyword">if</span> (key <span class="hljs-keyword">instanceof</span> DistroHttpCombinedKey) {  
toUpdateKeys = ((DistroHttpCombinedKey) key).getActualResourceTypes(); 
} <span class="hljs-keyword">else</span> {       
toUpdateKeys = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;(<span class="hljs-number">1</span>);  
toUpdateKeys.add(key.getResourceKey());      
}      
<span class="hljs-keyword">byte</span>[] queriedData = NamingProxy.getData(toUpdateKeys, key.getTargetServer()); 
<span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> DistroData(key, queriedData);
} <span class="hljs-keyword">catch</span> (Exception e)
{        
<span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> DistroException(String.format(<span class="hljs-string">"Get data from %s failed."</span>, key.getTargetServer()), e);    }
}
</code></pre>

<p><strong>示意图：</strong></p>

<ul>
<li><p>假设现在有 2 个节点，Nacos-A 是 A_SERVICE_XXX 服务的 Leader 节点，Nacos-B 是 B_SERVICE_XXX 服务的 Leader 节点；</p></li>

<li><p>Nacos-A 发送 CheckSum 请求时，将自己作为 Leader 的 A_SERVICE_XXX 分别计算 md5code；</p></li>

<li><p>md5code 生成规则：ip.getIp() + “:” + ip.getPort() + “<em>” + ip.getWeight() + “</em>” + ip.isHealthy() + “_” + ip.getClusterName()；</p></li>

<li><p>在 Nacos-B 中计算出有差异的 A_SERVICE_XXX，对于需要 Update 的从 Nacos-A 中进行全量数据拉取；对于需要 Remove 的从内存中删除。</p></li>
</ul>

<h3 id="5-5-v2-版本-verify">5.5 v2 版本 Verify</h3>

<ul>
<li><p>区别于 v1 版本的实现，v2 中以 ClientId 维度进行 CheckSum；</p></li>

<li><p>Nacos-1 对于本节点的所有 ClientId，每个 ClientId都包装成一个 Task 任务，使用 gRPC 发送给所有的 Distro 节点；</p></li>
</ul>

<pre><code class="language-Java hljs"><span class="hljs-meta">@Overridepublic</span> <span class="hljs-function">List&lt;DistroData&gt; <span class="hljs-title">getVerifyData</span><span class="hljs-params">()</span> </span>{ 
List&lt;DistroData&gt; result = <span class="hljs-keyword">new</span> LinkedList&lt;&gt;(); 
<span class="hljs-keyword">for</span> (String each : clientManager.allClientId()) 
{
Client client = clientManager.getClient(each);  
<span class="hljs-keyword">if</span> (<span class="hljs-keyword">null</span> == client || !client.isEphemeral()) 
{           
<span class="hljs-keyword">continue</span>;    
}       
<span class="hljs-keyword">if</span> (clientManager.isResponsibleClient(client)) {   
<span class="hljs-comment">// TODO add revision for client.   </span>
DistroClientVerifyInfo verifyData = <span class="hljs-keyword">new</span> DistroClientVerifyInfo(client.getClientId(), <span class="hljs-number">0</span>);   
DistroKey distroKey = <span class="hljs-keyword">new</span> DistroKey(client.getClientId(), TYPE);   
DistroData data = <span class="hljs-keyword">new</span> DistroData(distroKey,                                             ApplicationUtils.getBean(Serializer.class).serialize(verifyData));            data.setType(DataOperation.VERIFY);     
result.add(data);   
}   
}   
<span class="hljs-keyword">return</span> result;
}
</code></pre>

<ul>
<li><p>每个 ClientId 发送的校验 Version=1，Version 作为保留的扩展特性；</p></li>

<li><p>接收 Verify 请求的节点从请求参数中获取 ClientId，并检查自身是否有这个 Client，若此 Client 存在，则更新 Client 下的所有 Instance、以及 Client 自身的最新活跃时间为当前时间。</p></li>
</ul>

<h3 id="5-6-小结">5.6 小结</h3>

<p><strong>1. V1 Distro 最终数据一致性：</strong></p>

<ul>
<li><p>计算每个 Service 的 CheckSum 时，使用的是 ip.getIp() + “:” + ip.getPort() + “<em>” + ip.getWeight() + “</em>” + ip.isHealthy() + “_” + ip.getClusterName() 进行 CheckSum 计算；</p></li>

<li><p>对于需要更新的数据，向原节点全量拉取 Service 的数据；可以考虑优化成差量拉取。</p></li>
</ul>

<p><strong>2. V2 Distro 最终一致性：</strong> 每个节点以 ClientId 为维度进行集群广播，以 ClientId，Version=0 进行数据校验。</p>

<h2 id="part-6-sofaregistry">PART. 6 SOFARegistry</h2>

<h3 id="6-1-registry-数据读写">6.1 Registry 数据读写</h3>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d01edb5cd6ac4029886eb427b484cf58~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<ol>
<li><p>Client 发起服务注册数据 Publisher 给 SessionServer，SessionServer 接收成功；</p></li>

<li><p>SessionServer 接收到 Publisher 数据后，首先写入内存&nbsp; <em>(Client 发送过来的 Publisher 数据，SessionServer 都会存储到内存，用于后续可以跟 DataServer 做定期检查)</em> ，然后将 Publisher 数据发送给 DataServer，DataServer收到 Session 的 Pub 之后，修改 Datum 的版本号；</p></li>

<li><p>DataServer 先对 Notify 的请求做 merge 操作 <em>（等待 1000ms）</em> ，然后将数据的变更事件通知给所有 SessionServer <em>(事件内容是 ID&nbsp;和版本号信息和版本号信息：<datainfoid> 和 <version>)</version></datainfoid></em> ；</p></li>

<li><p>SessionServer 接收到变更事件通知后，对比 SessionServer 内存中存储的 DataInfoId 的 Version，发现比 DataServer 发过来的小，所以主动向 DataServer 获取&nbsp;DataInfoId 的数据，即获取具体的 Publisher 列表数据，获取数据成功后，创建 pushTask；</p></li>

<li><p>SessionServer 检测 pushTask 是否达到执行时间 <em>（T2+500MS）</em> ，对于达到执行时间的 pushTask，从队列中取出 Task，开始进行推送；</p></li>

<li><p>SessionServer 将数据推送给相应的 Client、Client Callback、SeesionServer 收到 ACK。</p></li>
</ol>

<h3 id="6-2-v6-秒级数据一致性">6.2 v6 秒级数据一致性</h3>

<p>详见 <em><a href="#">https://www.sofastack.tech/projects/sofa-registry/code-analyze/code-analyze-data-synchronization/</a></em> 本文不再重复描述。</p>

<h3 id="6-3-多机房数据一致性">6.3 多机房数据一致性</h3>

<p>在 6.2&nbsp;的同机房 Data-Leader 与 Data-Follower 数据同步的方案下，可以将这个方案进一步扩展到多机房之间的数据同步：</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/61c8b5d012504441be15939fc5ee4e18~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<p><strong>Meta 跨机房同步 SlotTable：</strong></p>

<ol>
<li><p>数据：本机房 SlotTable 数据；</p></li>

<li><p>通信：全量轮询；</p></li>

<li><p>DataCenterB Meta Leader 定时拉取到 DataCenterA 集群的 SlotTable 数据更新后，保存到本地 Meta Leader 内存中，然后通知给 DataCenterB 集群的&nbsp;Data 和 Session。</p></li>
</ol>

<p><strong>Data 跨机房同步 SlotData：</strong></p>

<ol>
<li><p>数据：每台 Data 同步自身 Slot Leader 的数据；</p></li>

<li><p>通信：增量通知+全量 DataInfoId 定时比对拉取；</p></li>

<li><p>Data-A1 和 Data-B2 从 Meta 获取到完整的 SlotTable 数据后，可以解析到自己是 SlotId=1 的 Leader 节点，需要进行数据同步；</p></li>

<li><p>当 Data-B2 中收到本机房 Session 的 Pub、ubPub、Client_off 请求后，完成本机房 Datum 数据处理；然后将 Datum.Version 通知给本机房 Session，同时将具体的 Pub、ubPub、Client_off 请求发送给 Data-A1；</p></li>

<li><p>Data-A1定时将 SlotId=1 的摘要数据发送给 Data-B2，将 SlotId=2 的摘要数据发送给 Data-B3，返回有差异的 DataInfoId 列表；再将差异 DataInfoId 进行性细的 Pub 摘要对比，确保数据最终一致；</p></li>

<li><p>Data-A1 将变化的 DataInfoId 以及 Datum Version 通知给本集群所有的 Session，将 DataCenterB 的数据变化推送给 DataCenterA 的所有 Client。</p></li>
</ol>

<h2 id="part-7-总结">PART. 7 总结</h2>

<p>最后我们对 SOFARegistry 和其它开源产品进行总结对比：</p>

<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/12863698bbe2433f8fa8bf4a128c0cf5~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" data-action="zoom"></p>

<p><strong>了解更多…</strong></p>

<p><strong>SOFARegistry Star 一下✨：</strong></p>

<p><em><a href="#">https://github.com/sofastack/sofa-registry/</a></em></p>

<p><strong>&nbsp;本周推荐阅读</strong></p>

<p><a href="#">SOFARegistry | 大规模集群优化实践</a></p>

<p><a href="#">SOFARegistry 源码｜数据同步模块解析</a></p>

<p><a href="#">SOFARegistry 源码｜数据分片之核心-路由表 SlotTable 剖析</a></p>

<p><a href="#">SOFAServerless 体系助力业务极速研发</a></p>

</article>
		<div class="-show-mobile">
			

	<nav class="ss-pagination-next">
		<a class="link-prev" href="/sofa2/blog/sofa-weekly-20221230/">
			<span class="text">上一篇: </span>
			<span class="text">SOFA Weekly | 2023 我们一起加油、本周 Contributor &amp; QA</span>
		</a>
		<a class="link-next" href="/sofa2/blog/sofaweekly-20230106/">
			<span class="text">下一篇: </span>
			<span class="text">SOFA Weekly | SOFANews、本周贡献 &amp; issue 精选</span>
		</a>
	</nav>

		</div>
	</main>

	<aside class="ss-layout-aside">
		
		


<div class="ss-card">
	<h2 class="card-title">
		相关推荐
	</h2>
	
	<ul class="ss-aside-related">
		
			<li><a href="/sofa2/blog/nydus-mirror-scan-acceleration/">Nydus 镜像扫描加速</a></li>
		
			<li><a href="/sofa2/blog/dragonfly-and-nydus-mirror-mode-integration-practice/">Dragonfly 和 Nydus Mirror 模式集成实践</a></li>
		
			<li><a href="/sofa2/blog/we-have-come-to-the-post-cloud-native-era-how-can-we-operate-and-maintain-on-a-large-scale/">已来到 “后云原生时代” 的我们，如何规模化运维？</a></li>
		
			<li><a href="/sofa2/blog/cost-reduction-and-efficiency-increase-ants-exploration-and-practice-in-sidecarless/">降本增效：蚂蚁在 Sidecarless 的探索和实践</a></li>
		
			<li><a href="/sofa2/blog/is-sidecarless-the-next-stop-for-servicemesh/">Service Mesh 的下一站是 Sidecarless 吗？</a></li>
		
	</ul>
	
</div>

		<div class="ss-aside-tags ss-card">
	<h2 class="card-title">
		标签
		<span class="card-extra">全部展开</span>
	</h2>
	<ul class="tag-list">
		
			<li class="tag"><a href="/sofa2/tags/1024/">1024</a></li>
		
			<li class="tag"><a href="/sofa2/tags/api-gateway/">API Gateway </a></li>
		
			<li class="tag"><a href="/sofa2/tags/cafedeployment/">CafeDeployment</a></li>
		
			<li class="tag"><a href="/sofa2/tags/cloud-native/">Cloud Native</a></li>
		
			<li class="tag"><a href="/sofa2/tags/cncf/">CNCF</a></li>
		
			<li class="tag"><a href="/sofa2/tags/db-mesh/">DB Mesh</a></li>
		
			<li class="tag"><a href="/sofa2/tags/dragonfly/">Dragonfly</a></li>
		
			<li class="tag"><a href="/sofa2/tags/elasticdl/">ElasticDL</a></li>
		
			<li class="tag"><a href="/sofa2/tags/http/3/">HTTP/3</a></li>
		
			<li class="tag"><a href="/sofa2/tags/http/quic/">HTTP/QUIC</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/kata-container/">Kata Container</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/kata-containers/">Kata Containers</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/knative/">Knative</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/kubecon/">KubeCon</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/kubernetes/">Kubernetes</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/meetup/">Meetup</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/mosn/">MOSN</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/nydus/">Nydus</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/occlum/">Occlum</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/rpc-%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%80%83%E5%92%8C%E9%87%8F/">RPC 框架设计的考和量</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/seata/">Seata</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/serverless/">Serverless</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/serverlesstask/">ServerlessTask</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/service-mesh/">Service Mesh</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/service-mesh-meetup/">Service Mesh Meetup</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/service-mesh-virtual-meetup/">Service Mesh Virtual Meetup</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/service-mesh-webinar/">Service Mesh Webinar</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/service-mesh-%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">Service Mesh 落地实践</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sidecar-%E5%AE%B9%E5%99%A8/">Sidecar 容器</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofa/">SOFA</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofa-weekly/">SOFA Weekly</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaacts/">SOFAActs</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaark/">SOFAArk</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaarklab/">SOFAArkLab</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofabolt/">SOFABolt</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaboot/">SOFABoot</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofachannel/">SOFAChannel</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofadashboard/">SOFADashboard</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaenclave/">SOFAEnclave</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofajraft/">SOFAJRaft</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofajraft-%E7%89%B9%E6%80%A7%E8%A7%A3%E6%9E%90/">SOFAJRaft 特性解析</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofalab/">SOFALab</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofalookout/">SOFALookout</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofameetup/">SOFAMeetup</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofamesh/">SOFAMesh</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofaregistry/">SOFARegistry</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofarpc/">SOFARPC</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofastack/">SOFAStack</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofatalk/">SOFATalk</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sofatracer/">SOFATracer</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/springboot/">SpringBoot</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/sqlflow/">SQLFlow</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/summer-2021/">Summer 2021</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/workshop/">Workshop</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/zsearch/">ZSearch</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/">云原生</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6/">任务调度</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E4%BD%A0%E7%9A%84%E5%BC%80%E6%BA%90%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/">你的开源入门指南</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/">分布式事务</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/">分布式架构</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofaark-%E6%BA%90%E7%A0%81/">剖析 | SOFAArk 源码 </a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofabolt-%E6%A1%86%E6%9E%B6/">剖析 | SOFABolt 框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofaboot-%E6%A1%86%E6%9E%B6/">剖析 | SOFABoot 框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofajraft-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">剖析 | SOFAJRaft 实现原理</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofaregistry-%E6%A1%86%E6%9E%B6/">剖析 | SOFARegistry 框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofarpc-%E6%A1%86%E6%9E%B6/">剖析 | SOFARPC 框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%89%96%E6%9E%90-sofatracer-%E6%A1%86%E6%9E%B6/">剖析 | SOFATracer 框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%AE%9E%E8%B7%B5/">实践</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%BC%80%E6%BA%90/">开源</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%BC%80%E6%BA%90%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B8%8E%E6%9C%AA%E6%9D%A5/">开源容器镜像加速服务的演进与未来</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/">弹性伸缩</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E6%96%87%E4%BB%B6%E5%92%8C%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F%E7%B3%BB%E7%BB%9F/">文件和镜像加速系统</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E6%99%BA%E8%83%BD%E7%9B%91%E6%8E%A7/">智能监控</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/">智能运维</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E6%BA%90%E5%88%9B%E4%BC%9A/">源创会</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/">“源码解析”</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E7%B1%BB%E9%9A%94%E7%A6%BB%E6%A1%86%E6%9E%B6/">类隔离框架</a></li>
		
			<li class="tag" style="display: none;"><a href="/sofa2/tags/%E9%95%9C%E5%83%8F/">镜像</a></li>
		
	</ul>
</div>
	</aside>
</div>



	


<footer class="ss-footer">
	<div class="container">
		<div class="links">
			
				<div class="cate">
					<h2 class="cate-title">资源</h2>
					
						<a class="link" href="#">Github</a>
					
						<a class="link" href="#">Gitee</a>
					
						<a class="link" href="#">示例</a>
					
				</div>
			
				<div class="cate">
					<h2 class="cate-title">社交媒体</h2>
					
						<a class="link" href="#">知乎专栏</a>
					
						<a class="link" href="#">新浪微博</a>
					
						<a class="link" href="#">Twitter</a>
					
				</div>
			
				<div class="cate">
					<h2 class="cate-title">参与进来</h2>
					
						<a class="link" href="#">反馈</a>
					
						<a class="link" href="#">社区</a>
					
						<a class="link" href="#">Wiki</a>
					
						<a class="link" href="#">Email</a>
					
						<a class="link" href="/sofa2/hr/">加入我们</a>
					
				</div>
			
				<div class="cate">
					<h2 class="cate-title">蚂蚁集团开源项目</h2>
					
						<a class="link" href="#">Ant Design</a>
					
						<a class="link" href="#">Egg </a>
					
						<a class="link" href="#">SQLFlow</a>
					
						<a class="link" href="#">更多</a>
					
				</div>
			
		</div>
		<div class="qrcode">
			
				<div>
					<img class="qrcode-img" src="/sofa2/img/qrcode/qrcode_video.png" href="/sofa2/img/qrcode/qrcode_video.png">
					<p class="qrcode-desc">微信视频号</p>
				</div>
			
				<div>
					<img class="qrcode-img" src="/sofa2/img/qrcode/qrcode_1.png" href="/sofa2/img/qrcode/qrcode_1.png">
					<p class="qrcode-desc">微信公众号</p>
				</div>
			
				<div>
					<img class="qrcode-img" src="/sofa2/img/qrcode/dingtalk_7.jpg" href="/sofa2/img/qrcode/dingtalk_7.jpg">
					<p class="qrcode-desc">钉钉群</p>
				</div>
			
		</div>
	</div>
	<div class="copyright">
		<p>
			© 2018 - 2022  The SOFAStack Authors
			<a href="#">浙 ICP 备 16045294 号-3</a>
		</p>
	</div>
</footer>



<svg id="SvgjsSvg1001" width="2" height="0" xmlns="http://www.w3.org/2000/svg" version="1.1" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:svgjs="http://svgjs.com/svgjs" focusable="false" style="overflow: hidden; top: -100%; left: -100%; position: absolute; opacity: 0;"><defs id="SvgjsDefs1002"></defs><polyline id="SvgjsPolyline1003" points="0,0"></polyline><path id="SvgjsPath1004" d="M0 0 "></path></svg></body></html>